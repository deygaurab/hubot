{"index": {"_id": 1}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "If EMR launch itself fails group execution won't happen. This causes Data freshness/Data availability issues.", "Cause": "When the EMR lauch fails", "Log snippet": "\n{  \n  \"Status\":\"FAILED\",\n  \"Task\":\"launch-cluster\",\n  \"Record_ID\":\"rec-fe5aam554jl5q\",\n  \"Reason\":\"TIME_OUT\",\n  \"Provisioned_Product_Name\":\"r3d3-emr-R3D3-QBO-US-ARAPCREDITPMNTCHARGELINKS-20190226010002\"\n}", "Action To Do": "1. Rerun the Failed Tidal job without any change.\n", "Page Dev": "No", "Alert Source": "Tidal"}
{"index": {"_id": 2}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "1. Tidal failure message contains the EMR information. Search for the key \"EMRName\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"Terminated clusters\".\n3. Go to \"Steps\" tab and open the stdout/stderr of \"Reconciler Job\" step.\n4. Search for \"error/exception\" in the stdout/stderr. If the failure cause is unavailability of dependent components, rerun the failed group by overriding the \"Command Parameters\" of failed job in the Tidal by appending \"-r\" option\n     Example : \n     Command line parameters of R3D3_QBO_US_GROUP_8 : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105'\n     To rerun this this group : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105 -r'\n", "Log snippet": "", "Action To Do": "`\"1. Tidal failure message contains the EMR information. Search for the key \"\"EMRName\"\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"\"Terminated clusters\"\".\n3. Go to \"\"Steps\"\" tab and open the stdout/stderr of \"\"Reconciler Job\"\" step.\n4. Search for \"\"error/exception\"\" in the stdout/stderr. If the failure cause is unavailability of dependent components, rerun the failed group by overriding the \"\"Command Parameters\"\" of failed job in the Tidal by appending \"\"-r\"\" option\n     Example : \n     Command line parameters of R3D3_QBO_US_GROUP_8 : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105'\n     To rerun this this group : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105 -r'\n\"", "Page Dev": "NO", "Alert Source": "TIDAL"}
{"index": {"_id": 3}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "https://issues.apache.org/jira/browse/SPARK-16764", "Log snippet": "java.lang.RuntimeException: Cannot reserve additional contiguous bytes in the vectorized reader (requested 406415514 bytes). As a workaround, you can disable the vectorized reader. For parquet file format, refer to spark.sql.parquet.enableVectorizedReader; for orc file format, refer to spark.sql.orc.enableVectorizedReader.\n    at org.apache.spark.sql.execution.vectorized.WritableColumnVector.throwUnsupportedException(WritableColumnVector.java:106)\n    at org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:92)\n    at org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:463)\n    at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:497)\n    at org.apache.spark.sql.execution.datasources.parquet.VectorizedPlainValuesReader.readBinary(VectorizedPlainValuesReader.java:167)\n    at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readBinarys(VectorizedRleValuesReader.java:434)", "Action To Do": "\"1. Tidal failure message contains the EMR information. Search for the key \"\"EMRName\"\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"\"Terminated clusters\"\".\n3. Go to \"\"Steps\"\" tab and open the stdout/stderr of \"\"Reconciler Job\"\" step.\n4. Search for \"\"List of tables failed are\"\" in the stdout/stderr and search for the same tables in \"\"Application History\"\".\n5. Click on the application hyperlink and navigate to \"\"Stages\"\". Click on the \"\"Failed\"\" stage.\n6. Click on the stderr of the Failed task and find the reason for the failure. If the reason is \"\"Cannot reserve additional contiguous bytes in the vectorized reader (requested 406415514 bytes). As a workaround, you can disable the vectorized reader. For parquet file format, refer to spark.sql.parquet.enableVectorizedReader; for orc file format, refer to spark.sql.orc.enableVectorizedReader\"\", disable the vectorized reader option.\n7. To disable the vectorized reader, use the rm-metadata-service with the following information\nspark_confs=\"\"{'spark.sql.parquet.enableVectorizedReader':'false'}\"\"  table_full_name\n8. After the update, rerun the failed Tidal group as mentioned above.\"", "Page Dev": "YES", "Alert Source": "Tidal"}
{"index": {"_id": 4}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "When the DynamoDB metadata and S3 objects are out of sync", "Log snippet": "\"com.amazon.ws.emr.hadoop.fs.consistency.exception.FileDeletedInMetadataNotFoundException: File 'cdc-ingest/materializedData/QBOC43~QBO_DATA~ITEMBUNDLEMAP_1' is marked as deleted in the metadata\n     at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:441)\n     at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:417)\n      \n\ncom.amazon.ws.emr.hadoop.fs.consistency.exception.ConsistencyException: Directory 'cdc-ingest/cdcStg/QBOC04~QBO_DATA~ACCOUNTINTERNALS_1' present in the metadata but not s3\n    at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:507)\n    at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:417)\n\n\"`", "Action To Do": "\"1. Tidal failure message contains the EMR information. Search for the key \"\"EMRName\"\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"\"Terminated clusters\"\".\n3. Go to \"\"Steps\"\" tab and open the stdout/stderr of \"\"Reconciler Job\"\" step.\n4. Search for \"\"List of tables failed are\"\" in the stdout/stderr and search for the same tables in \"\"Application History\"\".\n5. Click on the application hyperlink and navigate to \"\"Stages\"\". Click on the \"\"Failed\"\" stage.\n6. Click on the stderr of the Failed task and find the reason for the failure. If the reason is \"\"Consistency Exception\"\", rerun the emrfs sync command.\n7. To rerun the emrfs sync command, add a step action on the EMR r3d3-emr-R3D3-TEST. Easy way to do this is, clone the already existing step and update arguments.\n    emrfs sync <S3 Path> (S3 path should absolute and should include bucket name. Bucket name is s3://idl-cdc-ingest-raw-uw2-data-lake-prd)\n    Example \n    1. for running emrfs sync on command on the path /cdc-ingest/materializedData/QBOC32~QBO_DATA~GENERICFILES_1/1546050658000~1546056368510/,  \n       emrfs sync s3://idl-cdc-ingest-raw-uw2-data-lake-prd/cdc-ingest/materializedData/QBOC32~QBO_DATA~GENERICFILES_1/1546050658000~1546056368510\n    2. for running emrfs sync on command on the path cdc-ingest/cdcStg/QBOC04~QBO_DATA~ACCOUNTINTERNALS_1, \n       emrfs sync s3://idl-cdc-ingest-raw-uw2-data-lake-prd/cdc-ingest/materializedData/QBOC43~QBO_DATA~ITEMBUNDLEMAP_1\n     As per our observation, In general, emrfs sync command takes 15-20min.\n8. After the emrfs sync command, rerun the failed Tidal group as mentioned above.\"", "Page Dev": "No", "Alert Source": "Tidal"}
{"index": {"_id": 5}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "For all the other alerts, apart from the above ones, please rerun with -r option and page dev , if rerun fails", "Log snippet": "For all the other alerts, apart from the above ones, please rerun with -r option and page dev , if rerun fails", "Action To Do": "\"Rerun the failed group by overriding the \"\"Command Parameters\"\" of failed job in the Tidal by appending \"\"-r\"\" option\n     Example : \n     Command line parameters of R3D3_QBO_US_GROUP_8 : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105'\n     To rerun this this group : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105 -r'\n\nIf rerun also fails, page to Dev\"", "Page Dev": "Yes", "Alert Source": "Tidal"}
{"index": {"_id": 1}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "If EMR launch itself fails group execution won't happen. This causes Data freshness/Data availability issues.", "Cause": "When the EMR lauch fails", "Log snippet": "\n{  \n  \"Status\":\"FAILED\",\n  \"Task\":\"launch-cluster\",\n  \"Record_ID\":\"rec-fe5aam554jl5q\",\n  \"Reason\":\"TIME_OUT\",\n  \"Provisioned_Product_Name\":\"r3d3-emr-R3D3-QBO-US-ARAPCREDITPMNTCHARGELINKS-20190226010002\"\n}", "Action To Do": "1. Rerun the Failed Tidal job without any change.\n", "Page Dev": "No", "Alert Source": "Tidal"}
{"index": {"_id": 2}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "1. Tidal failure message contains the EMR information. Search for the key \"EMRName\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"Terminated clusters\".\n3. Go to \"Steps\" tab and open the stdout/stderr of \"Reconciler Job\" step.\n4. Search for \"error/exception\" in the stdout/stderr. If the failure cause is unavailability of dependent components, rerun the failed group by overriding the \"Command Parameters\" of failed job in the Tidal by appending \"-r\" option\n     Example : \n     Command line parameters of R3D3_QBO_US_GROUP_8 : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105'\n     To rerun this this group : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105 -r'\n", "Log snippet": "", "Action To Do": "`\"1. Tidal failure message contains the EMR information. Search for the key \"\"EMRName\"\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"\"Terminated clusters\"\".\n3. Go to \"\"Steps\"\" tab and open the stdout/stderr of \"\"Reconciler Job\"\" step.\n4. Search for \"\"error/exception\"\" in the stdout/stderr. If the failure cause is unavailability of dependent components, rerun the failed group by overriding the \"\"Command Parameters\"\" of failed job in the Tidal by appending \"\"-r\"\" option\n     Example : \n     Command line parameters of R3D3_QBO_US_GROUP_8 : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105'\n     To rerun this this group : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105 -r'\n\"", "Page Dev": "NO", "Alert Source": "TIDAL"}
{"index": {"_id": 3}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "https://issues.apache.org/jira/browse/SPARK-16764", "Log snippet": "java.lang.RuntimeException: Cannot reserve additional contiguous bytes in the vectorized reader (requested 406415514 bytes). As a workaround, you can disable the vectorized reader. For parquet file format, refer to spark.sql.parquet.enableVectorizedReader; for orc file format, refer to spark.sql.orc.enableVectorizedReader.\n    at org.apache.spark.sql.execution.vectorized.WritableColumnVector.throwUnsupportedException(WritableColumnVector.java:106)\n    at org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:92)\n    at org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:463)\n    at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:497)\n    at org.apache.spark.sql.execution.datasources.parquet.VectorizedPlainValuesReader.readBinary(VectorizedPlainValuesReader.java:167)\n    at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readBinarys(VectorizedRleValuesReader.java:434)", "Action To Do": "\"1. Tidal failure message contains the EMR information. Search for the key \"\"EMRName\"\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"\"Terminated clusters\"\".\n3. Go to \"\"Steps\"\" tab and open the stdout/stderr of \"\"Reconciler Job\"\" step.\n4. Search for \"\"List of tables failed are\"\" in the stdout/stderr and search for the same tables in \"\"Application History\"\".\n5. Click on the application hyperlink and navigate to \"\"Stages\"\". Click on the \"\"Failed\"\" stage.\n6. Click on the stderr of the Failed task and find the reason for the failure. If the reason is \"\"Cannot reserve additional contiguous bytes in the vectorized reader (requested 406415514 bytes). As a workaround, you can disable the vectorized reader. For parquet file format, refer to spark.sql.parquet.enableVectorizedReader; for orc file format, refer to spark.sql.orc.enableVectorizedReader\"\", disable the vectorized reader option.\n7. To disable the vectorized reader, use the rm-metadata-service with the following information\nspark_confs=\"\"{'spark.sql.parquet.enableVectorizedReader':'false'}\"\"  table_full_name\n8. After the update, rerun the failed Tidal group as mentioned above.\"", "Page Dev": "YES", "Alert Source": "Tidal"}
{"index": {"_id": 4}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "When the DynamoDB metadata and S3 objects are out of sync", "Log snippet": "\"com.amazon.ws.emr.hadoop.fs.consistency.exception.FileDeletedInMetadataNotFoundException: File 'cdc-ingest/materializedData/QBOC43~QBO_DATA~ITEMBUNDLEMAP_1' is marked as deleted in the metadata\n     at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:441)\n     at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:417)\n      \n\ncom.amazon.ws.emr.hadoop.fs.consistency.exception.ConsistencyException: Directory 'cdc-ingest/cdcStg/QBOC04~QBO_DATA~ACCOUNTINTERNALS_1' present in the metadata but not s3\n    at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:507)\n    at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:417)\n\n\"`", "Action To Do": "\"1. Tidal failure message contains the EMR information. Search for the key \"\"EMRName\"\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"\"Terminated clusters\"\".\n3. Go to \"\"Steps\"\" tab and open the stdout/stderr of \"\"Reconciler Job\"\" step.\n4. Search for \"\"List of tables failed are\"\" in the stdout/stderr and search for the same tables in \"\"Application History\"\".\n5. Click on the application hyperlink and navigate to \"\"Stages\"\". Click on the \"\"Failed\"\" stage.\n6. Click on the stderr of the Failed task and find the reason for the failure. If the reason is \"\"Consistency Exception\"\", rerun the emrfs sync command.\n7. To rerun the emrfs sync command, add a step action on the EMR r3d3-emr-R3D3-TEST. Easy way to do this is, clone the already existing step and update arguments.\n    emrfs sync <S3 Path> (S3 path should absolute and should include bucket name. Bucket name is s3://idl-cdc-ingest-raw-uw2-data-lake-prd)\n    Example \n    1. for running emrfs sync on command on the path /cdc-ingest/materializedData/QBOC32~QBO_DATA~GENERICFILES_1/1546050658000~1546056368510/,  \n       emrfs sync s3://idl-cdc-ingest-raw-uw2-data-lake-prd/cdc-ingest/materializedData/QBOC32~QBO_DATA~GENERICFILES_1/1546050658000~1546056368510\n    2. for running emrfs sync on command on the path cdc-ingest/cdcStg/QBOC04~QBO_DATA~ACCOUNTINTERNALS_1, \n       emrfs sync s3://idl-cdc-ingest-raw-uw2-data-lake-prd/cdc-ingest/materializedData/QBOC43~QBO_DATA~ITEMBUNDLEMAP_1\n     As per our observation, In general, emrfs sync command takes 15-20min.\n8. After the emrfs sync command, rerun the failed Tidal group as mentioned above.\"", "Page Dev": "No", "Alert Source": "Tidal"}
{"index": {"_id": 5}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "For all the other alerts, apart from the above ones, please rerun with -r option and page dev , if rerun fails", "Log snippet": "For all the other alerts, apart from the above ones, please rerun with -r option and page dev , if rerun fails", "Action To Do": "\"Rerun the failed group by overriding the \"\"Command Parameters\"\" of failed job in the Tidal by appending \"\"-r\"\" option\n     Example : \n     Command line parameters of R3D3_QBO_US_GROUP_8 : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105'\n     To rerun this this group : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105 -r'\n\nIf rerun also fails, page to Dev\"", "Page Dev": "Yes", "Alert Source": "Tidal"}
{"index": {"_id": 1}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "If EMR launch itself fails group execution won't happen. This causes Data freshness/Data availability issues.", "Cause": "When the EMR lauch fails", "Log snippet": "\n{  \n  \"Status\":\"FAILED\",\n  \"Task\":\"launch-cluster\",\n  \"Record_ID\":\"rec-fe5aam554jl5q\",\n  \"Reason\":\"TIME_OUT\",\n  \"Provisioned_Product_Name\":\"r3d3-emr-R3D3-QBO-US-ARAPCREDITPMNTCHARGELINKS-20190226010002\"\n}", "Action To Do": "1. Rerun the Failed Tidal job without any change.\n", "Page Dev": "No", "Alert Source": "Tidal"}
{"index": {"_id": 2}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "1. Tidal failure message contains the EMR information. Search for the key \"EMRName\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"Terminated clusters\".\n3. Go to \"Steps\" tab and open the stdout/stderr of \"Reconciler Job\" step.\n4. Search for \"error/exception\" in the stdout/stderr. If the failure cause is unavailability of dependent components, rerun the failed group by overriding the \"Command Parameters\" of failed job in the Tidal by appending \"-r\" option\n     Example : \n     Command line parameters of R3D3_QBO_US_GROUP_8 : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105'\n     To rerun this this group : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105 -r'\n", "Log snippet": "", "Action To Do": "`\"1. Tidal failure message contains the EMR information. Search for the key \"\"EMRName\"\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"\"Terminated clusters\"\".\n3. Go to \"\"Steps\"\" tab and open the stdout/stderr of \"\"Reconciler Job\"\" step.\n4. Search for \"\"error/exception\"\" in the stdout/stderr. If the failure cause is unavailability of dependent components, rerun the failed group by overriding the \"\"Command Parameters\"\" of failed job in the Tidal by appending \"\"-r\"\" option\n     Example : \n     Command line parameters of R3D3_QBO_US_GROUP_8 : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105'\n     To rerun this this group : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105 -r'\n\"", "Page Dev": "NO", "Alert Source": "TIDAL"}
{"index": {"_id": 3}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "https://issues.apache.org/jira/browse/SPARK-16764", "Log snippet": "java.lang.RuntimeException: Cannot reserve additional contiguous bytes in the vectorized reader (requested 406415514 bytes). As a workaround, you can disable the vectorized reader. For parquet file format, refer to spark.sql.parquet.enableVectorizedReader; for orc file format, refer to spark.sql.orc.enableVectorizedReader.\n    at org.apache.spark.sql.execution.vectorized.WritableColumnVector.throwUnsupportedException(WritableColumnVector.java:106)\n    at org.apache.spark.sql.execution.vectorized.WritableColumnVector.reserve(WritableColumnVector.java:92)\n    at org.apache.spark.sql.execution.vectorized.WritableColumnVector.appendBytes(WritableColumnVector.java:463)\n    at org.apache.spark.sql.execution.vectorized.OnHeapColumnVector.putByteArray(OnHeapColumnVector.java:497)\n    at org.apache.spark.sql.execution.datasources.parquet.VectorizedPlainValuesReader.readBinary(VectorizedPlainValuesReader.java:167)\n    at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readBinarys(VectorizedRleValuesReader.java:434)", "Action To Do": "\"1. Tidal failure message contains the EMR information. Search for the key \"\"EMRName\"\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"\"Terminated clusters\"\".\n3. Go to \"\"Steps\"\" tab and open the stdout/stderr of \"\"Reconciler Job\"\" step.\n4. Search for \"\"List of tables failed are\"\" in the stdout/stderr and search for the same tables in \"\"Application History\"\".\n5. Click on the application hyperlink and navigate to \"\"Stages\"\". Click on the \"\"Failed\"\" stage.\n6. Click on the stderr of the Failed task and find the reason for the failure. If the reason is \"\"Cannot reserve additional contiguous bytes in the vectorized reader (requested 406415514 bytes). As a workaround, you can disable the vectorized reader. For parquet file format, refer to spark.sql.parquet.enableVectorizedReader; for orc file format, refer to spark.sql.orc.enableVectorizedReader\"\", disable the vectorized reader option.\n7. To disable the vectorized reader, use the rm-metadata-service with the following information\nspark_confs=\"\"{'spark.sql.parquet.enableVectorizedReader':'false'}\"\"  table_full_name\n8. After the update, rerun the failed Tidal group as mentioned above.\"", "Page Dev": "YES", "Alert Source": "Tidal"}
{"index": {"_id": 4}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "When the DynamoDB metadata and S3 objects are out of sync", "Log snippet": "\"com.amazon.ws.emr.hadoop.fs.consistency.exception.FileDeletedInMetadataNotFoundException: File 'cdc-ingest/materializedData/QBOC43~QBO_DATA~ITEMBUNDLEMAP_1' is marked as deleted in the metadata\n     at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:441)\n     at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:417)\n      \n\ncom.amazon.ws.emr.hadoop.fs.consistency.exception.ConsistencyException: Directory 'cdc-ingest/cdcStg/QBOC04~QBO_DATA~ACCOUNTINTERNALS_1' present in the metadata but not s3\n    at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:507)\n    at com.amazon.ws.emr.hadoop.fs.consistency.ConsistencyCheckerS3FileSystem.getFileStatus(ConsistencyCheckerS3FileSystem.java:417)\n\n\"`", "Action To Do": "\"1. Tidal failure message contains the EMR information. Search for the key \"\"EMRName\"\", and find the EMR name.\n2. Log in to production DataLake account and search for the above EMR in \"\"Terminated clusters\"\".\n3. Go to \"\"Steps\"\" tab and open the stdout/stderr of \"\"Reconciler Job\"\" step.\n4. Search for \"\"List of tables failed are\"\" in the stdout/stderr and search for the same tables in \"\"Application History\"\".\n5. Click on the application hyperlink and navigate to \"\"Stages\"\". Click on the \"\"Failed\"\" stage.\n6. Click on the stderr of the Failed task and find the reason for the failure. If the reason is \"\"Consistency Exception\"\", rerun the emrfs sync command.\n7. To rerun the emrfs sync command, add a step action on the EMR r3d3-emr-R3D3-TEST. Easy way to do this is, clone the already existing step and update arguments.\n    emrfs sync <S3 Path> (S3 path should absolute and should include bucket name. Bucket name is s3://idl-cdc-ingest-raw-uw2-data-lake-prd)\n    Example \n    1. for running emrfs sync on command on the path /cdc-ingest/materializedData/QBOC32~QBO_DATA~GENERICFILES_1/1546050658000~1546056368510/,  \n       emrfs sync s3://idl-cdc-ingest-raw-uw2-data-lake-prd/cdc-ingest/materializedData/QBOC32~QBO_DATA~GENERICFILES_1/1546050658000~1546056368510\n    2. for running emrfs sync on command on the path cdc-ingest/cdcStg/QBOC04~QBO_DATA~ACCOUNTINTERNALS_1, \n       emrfs sync s3://idl-cdc-ingest-raw-uw2-data-lake-prd/cdc-ingest/materializedData/QBOC43~QBO_DATA~ITEMBUNDLEMAP_1\n     As per our observation, In general, emrfs sync command takes 15-20min.\n8. After the emrfs sync command, rerun the failed Tidal group as mentioned above.\"", "Page Dev": "No", "Alert Source": "Tidal"}
{"index": {"_id": 5}}
{"Alert": "TIDAL : AWS R3D3 Failed job - <GROUP NAME>", "Severity": "CRITICAL", "Impact": "Failure of a group could lead to Data freshness/Data availability issues.\n", "Cause": "For all the other alerts, apart from the above ones, please rerun with -r option and page dev , if rerun fails", "Log snippet": "For all the other alerts, apart from the above ones, please rerun with -r option and page dev , if rerun fails", "Action To Do": "\"Rerun the failed group by overriding the \"\"Command Parameters\"\" of failed job in the Tidal by appending \"\"-r\"\" option\n     Example : \n     Command line parameters of R3D3_QBO_US_GROUP_8 : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105'\n     To rerun this this group : prd r3d3 R3D3-QBO-US-GROUP-8 '-g R3D3_QBO_US_GROUP_8 -p 105 -r'\n\nIf rerun also fails, page to Dev\"", "Page Dev": "Yes", "Alert Source": "Tidal"}
